# -*- coding: utf-8 -*-
"""huggingFace.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vaeBtuPokMFFdOWo3AgBE8_Llwv_0YX6
"""

# !pip install --upgrade --force-reinstall langchain langchain-community langchain-openai langchain-core langchain-text-splitters faiss-cpu beautifulsoup4 langchain-huggingface
# !pip check

# !pip install chromadb

import bs4
from langchain_community.document_loaders import WebBaseLoader

# Only keep post title, headers, and content from the full HTML.
bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

assert len(docs) == 1
print(f"Total characters: {len(docs[0].page_content)}")

print(docs[0].page_content[:500])

from langchain_text_splitters import RecursiveCharacterTextSplitter

# with open("state_of_the_union.txt") as f:
#     state_of_the_union = f.read()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs)
print("first test:",all_splits[3])
print("second test: ",all_splits[4])

from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from getpass import getpass
# The existing token is likely invalid or lacks permissions, so I'm commenting it out.
# os.environ["HUGGINGFACEHUB_API_TOKEN"] = ""

# get your free access token from HuggingFace and paste it here
HF_token = getpass()
os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_token

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
vectorstore = Chroma.from_documents(all_splits, embeddings)

# from langchain_community.vectorstores import FAISS

# vectorstore = FAISS.from_texts(
#     [doc.page_content for doc in all_splits],
#     embedding=embeddings,
# )

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

# Retrieve the most similar text
# retrieved_documents = retriever.invoke("What is LangChain?")

# Retrieve documents with similarity scores
retrieved_with_scores = vectorstore.similarity_search_with_score("What is Agent System ", k=2)

# Print the retrieved documents and their scores
for doc, score in retrieved_with_scores:
    print(f"Content: {doc.page_content[:200]}...\nScore: {score}\n---")

"""### augment https://readmedium.com/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7"""

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers import AutoTokenizer, pipeline
# Create a tokenizer object by loading the pretrained "Intel/dynamic_tinybert" tokenizer.
#tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

# Create a question-answering model object by loading the pretrained "Intel/dynamic_tinybert" model.
# model = AutoModelForQuestionAnswering.from_pretrained("Intel/dynamic_tinybert")
model = AutoModelForQuestionAnswering.from_pretrained("google/flan-t5-small")

from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM # Import AutoModelForSeq2SeqLM

# Specify the model name you want to use
model_name = "google/flan-t5-small"

# Load the tokenizer associated with the specified model
tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)

# Load the model specifically for text-to-text generation
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define a text2text-generation pipeline using the model and tokenizer
text_generator = pipeline(
    "text2text-generation", # Changed from "question-answering" to "text2text-generation"
    model=model, # Use the loaded model
    tokenizer=tokenizer,
    #max_new_tokens=512 # Set max_new_tokens for generation
)

# Create an instance of the HuggingFacePipeline, which wraps the text-generation pipeline
llm = HuggingFacePipeline(
    pipeline=text_generator,
    model_kwargs={"temperature": 0}, # Temperature can still be set here
)

from langchain_classic.chains import RetrievalQA

retrievalQA = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,

)

query = "What is ANNOY?"
response = retrievalQA.invoke({"query": query})
print(f"\nQuestion: {query}")
print(f"\nAnswer: {response['result']}")