# -*- coding: utf-8 -*-
"""finetuningfiqa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TZ2pDjEyuOy6EhpFZTD2zuo_a5lftlUf
"""

!pip install datasets==3.6.0

# os.environ["HUGGINGFACEHUB_API_TOKEN"] = ""
import os
from getpass import getpass
HF_token = getpass()
os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_token

!pip install beir

# from beir import util, LoggingHandler
# from beir.datasets.data_loader import GenericDataLoader
# import logging
# import pathlib

# # Setup logging
# logging.basicConfig(format='%(asctime)s - %(message)s',
#                     level=logging.INFO,
#                     handlers=[LoggingHandler()])


# url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip"
# data_path = util.download_and_unzip(url, "datasets")

# corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split="train")

# print(f"Corpus size: {len(corpus)}")
# print(f"Queries size: {len(queries)}")
# print(f"Qrels size: {len(qrels)}")

# corpus_first_key = next(iter(corpus))
# print(f"Corpus entry for key '{corpus_first_key}': {corpus[corpus_first_key]}")
# print(f"Queries entry for key '0': {queries['0']}")
# print(f"Qrels entry for key '0': {qrels['0']}")

# query_id = '0'
# doc_id = '18850'

# print(f"Query content (ID: {query_id}): {queries[query_id]}")
# print(f"Document content (ID: {doc_id}): {corpus[doc_id]}")

"""###corpus"""

import datasets
from datasets import load_dataset
print(datasets.__version__)

dataset = load_dataset("BeIR/fiqa", name="corpus", split="corpus")

"""https://ir-datasets.com/beir#beir/fiqa/train"""

# from huggingface_hub import list_datasets
# datasets_list = list_datasets()
# print("fiqa" in datasets_list)

dataset

dataset = dataset.remove_columns(['title'])

dataset



dataset[0]

"""###queries"""

queries_dataset = load_dataset("BeIR/fiqa", "queries", split="queries")

queries_dataset

queries_dataset = queries_dataset.remove_columns(['title'])

queries_dataset

queries_dataset[0]

"""####mapping queries with corpus"""

ds = load_dataset("BeIR/fiqa-qrels")

print(f"{ds['train'][0]}")
print(len(ds['train']))

print(f"")

print(f" Documents: {len(dataset)}")
print(f"Queries: {len(queries_dataset)}")

qrels = load_dataset("BeIR/fiqa", name="qrels")

print(f"query: {queries[1][:200]}...")
print(f"document: {documents[1][:200]}...")



"""Split Data into Training and Validation Sets

Load Tokenizer and Model

https://stackoverflow.com/questions/75172073/how-to-use-automodelforsequenceclassification-for-multiclass-classification
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

print(" model and tokenizer loaded successfully!")
print(f"   model: {model_name}")

"""###https://huggingface.co/transformers/v4.7.0/preprocessing.html

###https://torchtext.readthedocs.io/en/latest/data.html
###https://www.geeksforgeeks.org/deep-learning/how-do-you-use-pytorchs-dataset-and-dataloader-classes-for-custom-data/
###https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00/
"""

print(f"Training dataset size: {len(train_dataset)}")
print(f"Validation dataset size: {len(val_dataset)}")

!pip install --upgrade transformers

qrels_train_split = ds['train']
print(f"Loaded qrels_train_split with {len(qrels_train_split)} entries.")

qrels_train_split = ds['train']
print(f"Loaded qrels_train_split with {len(qrels_train_split)} entries.")

training_pairs = []
labels = []

# Create a dictionary of queries and documents
queries_dict = {str(item['_id']): item['text'] for item in queries_dataset}
documents_dict = {str(item['_id']): item['text'] for item in dataset}

for entry in qrels_train_split:
    query_id = str(entry['query-id'])
    doc_id = str(entry['corpus-id'])
    score = entry['score']

    if query_id in queries_dict and doc_id in documents_dict:
        query_text = queries_dict[query_id]
        document_text = documents_dict[doc_id]

        training_pairs.append({'query': query_text, 'document': document_text})
        labels.append(score)

print(f"Generated {len(training_pairs)} training pairs.")
print(f"Generated {len(labels)} labels.")

print(f"{training_pairs[2]}")
print(f"{labels[0]}")



relevant_docs_per_query = {}

for entry in qrels_train_split:
    query_id = str(entry['query-id'])
    corpus_id = str(entry['corpus-id'])
    if query_id not in relevant_docs_per_query:
        relevant_docs_per_query[query_id] = set()
    relevant_docs_per_query[query_id].add(corpus_id)

print(f"Populated relevant_docs_per_query with {len(relevant_docs_per_query)} unique queries.")

"""## Extract All Corpus Document IDs

### Subtask:
Create a list containing all available `_id`s from the `dataset` (corpus). This list will be used for random sampling of documents.

**Reasoning**:
To extract all document IDs from the dataset, I will iterate through the dataset and append each item's '_id' to a new list.
"""

all_corpus_doc_ids = []
for item in dataset:
    all_corpus_doc_ids.append(item['_id'])

print(f"Total unique corpus document IDs: {len(all_corpus_doc_ids)}")

"""**Reasoning**:
Now that we have extracted all corpus document IDs, the next step is to generate an equal number of negative training samples as positive samples. This involves iterating through the positive qrels, randomly selecting a non-relevant document for each query, and adding these as negative pairs.


"""

import random

num_positive_samples = len(training_pairs)
negative_samples_count = 0

# Generate negative samples equal to the number of positive samples
while negative_samples_count < num_positive_samples:

    random_positive_entry = random.choice(qrels_train_split)
    query_id = str(random_positive_entry['query-id'])

    negative_doc_id = random.choice(all_corpus_doc_ids)


    if query_id in relevant_docs_per_query and negative_doc_id not in relevant_docs_per_query[query_id]:
        # Ensure the query and document actually exist in our dicts before adding
        if query_id in queries_dict and negative_doc_id in documents_dict:
            query_text = queries_dict[query_id]
            document_text = documents_dict[negative_doc_id]

            training_pairs.append({'query': query_text, 'document': document_text})
            labels.append(0) # Label 0 for negative sample
            negative_samples_count += 1

print(f"Generated {len(training_pairs)} total training pairs (positive + negative).")
print(f"Generated {len(labels)} total labels.")

training_pairs = []
labels = []

# Limit to the first 5000 relevant pairs as positive samples
num_positive_samples_to_generate = 5000

for i, entry in enumerate(qrels_train_split):
    if i >= num_positive_samples_to_generate:
        break

    query_id = str(entry['query-id'])
    doc_id = str(entry['corpus-id'])

    # Ensure the query and document IDs exist in the dictionaries
    if query_id in queries_dict and doc_id in documents_dict:
        query_text = queries_dict[query_id]
        document_text = documents_dict[doc_id]

        training_pairs.append({'query': query_text, 'document': document_text})
        labels.append(1) # Label 1 for positive sample

print(f"Generated {len(training_pairs)} positive training pairs.")

import random

# 5000 negative samples
num_negative_samples_to_generate = 5000
negative_samples_count = 0

while negative_samples_count < num_negative_samples_to_generate:

    random_positive_entry = random.choice(qrels_train_split)
    query_id = str(random_positive_entry['query-id'])


    negative_doc_id = random.choice(all_corpus_doc_ids)


    if query_id in relevant_docs_per_query and negative_doc_id not in relevant_docs_per_query[query_id]:
        if query_id in queries_dict and negative_doc_id in documents_dict:
            query_text = queries_dict[query_id]
            document_text = documents_dict[negative_doc_id]

            training_pairs.append({'query': query_text, 'document': document_text})
            labels.append(0)
            negative_samples_count += 1

print(f"Generated {len(training_pairs) - num_negative_samples_to_generate} positive training pairs and {negative_samples_count} negative training pairs.")
print(f"Total training pairs: {len(training_pairs)}")
print(f"Total labels: {len(labels)}")

from sklearn.model_selection import train_test_split
train_pairs, val_pairs, train_labels, val_labels = train_test_split(
    training_pairs, labels, test_size=0.2, random_state=42
)

print(f"   Training samples: {len(train_pairs)}")
print(f"   Validation samples: {len(val_pairs)}")

import torch

train_encodings = tokenizer(
    [pair['query'] for pair in train_pairs],
    [pair['document'] for pair in train_pairs],
    truncation=True,
    padding=True,
    max_length=512
)

import torch
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

train_encodings = tokenizer(
    [pair['query'] for pair in train_pairs],
    [pair['document'] for pair in train_pairs],
    truncation=True,
    padding=True,
    max_length=512
)

val_encodings = tokenizer(
    [pair['query'] for pair in val_pairs],
    [pair['document'] for pair in val_pairs],
    truncation=True,
    padding=True,
    max_length=512
)

import torch

class RelevanceDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

train_dataset = RelevanceDataset(train_encodings, train_labels)
val_dataset = RelevanceDataset(val_encodings, val_labels)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./financial_reranker',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to=[]
)


from transformers import AutoModelForSequenceClassification, Trainer


model_name = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)


trainer.train()

