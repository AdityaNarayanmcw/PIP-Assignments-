# -*- coding: utf-8 -*-
"""finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oiIpG99zNsqi9zz1NxhO7ZmW_-RLo26g

Task 2: Fine-Tuning a Transformer Model (Multiclass Emotion Classification) Task Description: Fine-tune the distilbert-base-uncased model for multiclass text classification. Your task is to train a model to predict one of six emotions (sadness, joy, love, anger, fear, surprise) based on the text. This task requires adapting the standard binary classification workflow to a multiclass problem.


Data Source:
Dataset: emotion (also known as dair-ai/emotion)

Source: Load directly from the Hugging Face datasets library using load_dataset("emotion").

Expectations: Your submitted script should be a complete, runnable training file that:
· Loads the emotion dataset.

· Loads the AutoTokenizer for distilbert-base-uncased.

· Loads AutoModelForSequenceClassification from distilbert-base-uncased.

· Defines a preprocessing function that tokenizes the text column. (Note: The label column in this dataset is already in the correct integer format, so no mapping is needed).

· Applies this preprocessing function to the dataset using .map().

· Defines a DataCollatorWithPadding.

· Defines TrainingArguments to configure the training process.

· Instantiates the Trainer with the model, arguments, datasets, tokenizer, and data collator.

· Calls trainer.train() to begin the fine-tuning process.

Dataset: emotion (also known as dair-ai/emotion)
Source: Load directly from the Hugging Face datasets library using load_dataset("emotion").
· Loads the emotion dataset.
source:
https://huggingface.co/docs/hub/datasets-usage
https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb#scrollTo=UmvbnJ9JIrJd
"""

from datasets import load_dataset

ds = load_dataset("emotion")

ds['train'][2]
ds

"""· Loads the AutoTokenizer for distilbert-base-uncased.

· Loads AutoModelForSequenceClassification from distilbert-base-uncased.
source:
https://huggingface.co/distilbert/distilbert-base-uncased, https://stackoverflow.com/questions/71037800/huggingface-fine-tuning-distilbert-base-uncased-and-then-using-pipeline-throws-e
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
num_labels=6
model_name='distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
)

"""· Defines a preprocessing function that tokenizes the text column. (Note: The label column in this dataset is already in the correct integer format, so no mapping is needed).
https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/sequence_classification.ipynb#scrollTo=nMgMGvg8W52Y
"""

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)

"""· Applies this preprocessing function to the dataset using .map().
https://huggingface.co/docs/datasets/v4.4.1/en/package_reference/main_classes#datasets.Dataset.map
"""

tokenized_ds = ds.map(preprocess_function, batched=True)

"""· Defines a DataCollatorWithPadding.

https://huggingface.co/docs/transformers/main_classes/data_collator, https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding

"""

from transformers import DataCollatorWithPadding

data_collator=DataCollatorWithPadding(tokenizer=tokenizer)

"""· Defines TrainingArguments to configure the training process.
https://huggingface.co/docs/transformers/v4.57.1/en/main_classes/trainer#transformers.TrainingArguments
"""

from transformers import TrainingArguments

training_args=TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=4,
    weight_decay=0.01,
    warmup_steps=500,
    logging_dir="./logs",
    logging_steps=500,
    eval_strategy="epoch",
    save_strategy='epoch',
    load_best_model_at_end=True,
    push_to_hub=False,
    report_to=[]
)

"""· Instantiates the Trainer with the model, arguments, datasets, tokenizer, and data collator.
https://huggingface.co/docs/transformers/main_classes/trainer
"""

from transformers import Trainer

trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

"""· Calls trainer.train() to begin the fine-tuning process.
https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.model
"""

trainer.train()

test_results = trainer.evaluate(tokenized_ds['test'])
test_results

# trainer.save_model("./emotion_classifier_final")
# tokenizer.save_pretrained("./emotion_classifier_final")

"""#mapping labels to emotions according to dair-ai/emotion labels"""

# emotion_labels = {0: "sadness",1: "joy",2: "love",3: "anger",4: "fear",5: "surprise"}

"""# some test cases"""

# test_texts = ["I am so happy today!","This makes me really angry.","I feel so sad and lonely."]

# import numpy as np

# for text in test_texts:
#     inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
#     inputs = {k: v.to(model.device) for k, v in inputs.items()} # Move inputs to the model's device
#     outputs = model(**inputs)
#     prediction = np.argmax(outputs.logits.detach().cpu().numpy(), axis=-1)[0] # Move logits back to CPU for numpy conversion

#     print(f"Text: {text}")
#     print(f"Predicted Emotion: {emotion_labels[prediction]}")