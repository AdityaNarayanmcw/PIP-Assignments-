# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vh7JzCtyJKQ1rS4KfArCvYPoC1Oz0T3D
"""

!pip install --upgrade --force-reinstall langchain langchain-community langchain-openai langchain-core langchain-text-splitters faiss-cpu beautifulsoup4
!pip check

# !pip install --upgrade langchain
# !langchain-core
# !

"""source: https://docs.langchain.com/oss/python/langchain/retrieval#2-step-rag

#路 Load: Use a DocumentLoader (e.g., WebBaseLoader) to load the text content from the URL.
source: https://pypi.org/project/langchain-text-splitters/
"""

# !pip install --upgrade langchain-text-splitters

# !pip install -qU \
#   langchain==0.3.25 \
#   langchain-huggingface==0.3.0 \
#   langchain-openai==0.3.22

# !pip install --upgrade langchain

"""https://docs.langchain.com/oss/python/integrations/document_loaders/web_base"""

import bs4
from langchain_community.document_loaders import WebBaseLoader

# Only keep post title, headers, and content from the full HTML.
bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

assert len(docs) == 1
print(f"Total characters: {len(docs[0].page_content)}")

"""#Load"""

print(docs[0].page_content[:500])

len(docs)

"""#路 Split: Use a TextSplitter (e.g., RecursiveCharacterTextSplitter) to break the document into smaller chunks.
source: https://docs.langchain.com/oss/python/integrations/splitters

!pip install -qU langchain-text-splitters
"""

# !pip install -qU langchain-text-splitters

"""#source:
https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter,

https://synthmetric.com/document-chunking-size-overlap-and-what-actually-works/

https://stackoverflow.com/questions/76681318/why-is-recursivecharactertextsplitter-not-giving-any-chunk-overlap
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# with open("state_of_the_union.txt") as f:
#     state_of_the_union = f.read()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs)
print("first test:",all_splits[3])
print("second test: ",all_splits[4])

print(f"End of chunk 3 (last 150 chars):\n{all_splits[3].page_content[-500:]}")
print(f"\nBeginning of chunk 4 (first 150 chars):\n{all_splits[4].page_content[:500]}")

"""###路 Embed & Store: Use an embedding model and an in-memory vector store (e.g., FAISS or Chroma) to create a searchable index of the chunks.
source : https://docs.langchain.com/oss/python/integrations/text_embedding
"""

# !pip install -qU  langchain langchain-huggingface sentence_transformers

# !pip install -qU langchain-openai
# #from langchain_huggingface.embeddings import HuggingFaceEmbeddings

from langchain_openai import OpenAIEmbeddings
import getpass
import os

os.environ["OPENAI_API_KEY"] = "openai-key"
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",
    # With the `text-embedding-3` class
    # of models, you can specify the size
    # of the embeddings you want returned.
    # dimensions=1024
)
#embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

"""###https://docs.langchain.com/oss/python/integrations/vectorstores/faiss"""

from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_texts(
    [doc.page_content for doc in all_splits],
    embedding=embeddings,
)

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

# Retrieve the most similar text
# retrieved_documents = retriever.invoke("What is LangChain?")

# Retrieve documents with similarity scores
retrieved_with_scores = vectorstore.similarity_search_with_score("What is Agent System ", k=4)

# Print the retrieved documents and their scores
for doc, score in retrieved_with_scores:
    print(f"Content: {doc.page_content[:200]}...\nScore: {score}\n---")

"""### source : https://docs.langchain.com/oss/python/integrations/vectorstores/faiss
https://docs.langchain.com/oss/python/langchain/rag#faiss

### source: https://stackoverflow.com/questions/58957169/faiss-error-could-not-find-a-version-that-satisfies-the-requirement-faiss-from
"""

# !pip install faiss-gpu-cu12
# #!python --version

"""### 路 Retrieve & Generate: Create a RetrievalQA chain (or equivalent) that connects a retriever (from your vector store) and an LLM (model of your choice) to answer questions."""

!python -c "import langchain, langchain_core; print('langchain', getattr(langchain,'__version__',None)); print('langchain_core', getattr(langchain_core,'__version__',None))"

from langchain_openai import ChatOpenAI
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

!pip show langchain

# !pip install langchain
!# run this in a notebook cell, then restart the kernel/runtime
# !pip install -U langchain-classic langchain-openai

"""###https://stackoverflow.com/questions/79807773/using-create-retrieval-chain-due-to-retrievalqa-deprecation"""

# Source - https://stackoverflow.com/a
# Posted by cottontail
# Retrieved 2025-11-16, License - CC BY-SA 4.0

from langchain_classic.chains import RetrievalQA
from langchain_openai import ChatOpenAI
query = "What is an agent system and what are its key components?"
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
retrievalQA = RetrievalQA.from_llm(llm=llm,
                                   retriever=retriever,
                                   return_source_documents=True,  # Return retrieved documents
    verbose=True)
response = retrievalQA.invoke(query)

print(response['result'])

for i, doc in enumerate(response['source_documents'], 1):
    print(f"\n--- Document {i} ---")
    print(f"Content: {doc.page_content[:300]}...")
    print(f"Length: {len(doc.page_content)} characters")
    if hasattr(doc, 'metadata'):
        print(f"Metadata: {doc.metadata}")

while True:
    user_query = input("Your question: ").strip()

    if not user_query:
        continue

    if user_query.lower() in ['quit', 'exit', 'q']:
        print("Goodbye!")
        break

    response = retrievalQA.invoke(user_query)
    print(f"\nAnswer: {response['result']}\n")